{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 21:27:46.998362: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-14 21:27:47.012345: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739557667.029622 3582209 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739557667.034811 3582209 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-14 21:27:47.052867: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from lib import Datasets\n",
    "from datasets import load_dataset\n",
    "import copy\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence, List, Literal\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "\n",
    "# PROMPT = (\n",
    "#         \"Below is an instruction that describes a task. \"\n",
    "#         \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "#         \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "#     )\n",
    "\n",
    "PROMPT = (\n",
    "        \"### Task:\\n{instruction}\\n\\n### Solution:\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"distilgpt2\"\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e58ec8c3ebb487989ef2b14f5b7feaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# name = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "model = AutoModelForCausalLM.from_pretrained(name, torch_dtype=torch.float16, device_map='balanced')\n",
    "\n",
    "# model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --data_path meta-math/MetaMathQA \\\n",
    "# --dataset_field query response \\\n",
    "    \n",
    "# python -u train_model.py \\\n",
    "#     --model_name_or_path $BASE_MODEL \\\n",
    "#     --output_dir $OUTPUT \\\n",
    "#     --corda_mode False \\\n",
    "#     --lora_r 128 \\\n",
    "#     --data_path meta-math/MetaMathQA \\\n",
    "#     --dataset_split \"train[:100000]\" \\\n",
    "#     --dataset_field query response \\\n",
    "#     --num_train_epochs 1 \\\n",
    "#     --per_device_train_batch_size 1 \\\n",
    "#     --gradient_accumulation_steps 128 \\\n",
    "#     --save_strategy \"steps\" \\\n",
    "#     --save_steps 100 \\\n",
    "#     --save_total_limit 1 \\\n",
    "#     --learning_rate 2e-5 \\\n",
    "#     --weight_decay 0. \\\n",
    "#     --warmup_ratio 0.03 \\\n",
    "#     --lr_scheduler_type \"cosine\" \\\n",
    "#     --logging_steps 1 \\\n",
    "#     --bf16 True \\\n",
    "#     --tf32 True \\\n",
    "#     --report_to none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    \n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        # print(len(label), source_len)\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "        \n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "def train_tokenize_function(examples, tokenizer, query, response):\n",
    "    sources = [PROMPT.format_map(dict(instruction=instruction)) for instruction in examples[query]]\n",
    "    targets = [f\"{output}{tokenizer.eos_token}\" for output in examples[response]]\n",
    "    data_dict = preprocess(sources, targets, tokenizer)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        name,\n",
    "        model_max_length=512,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# tokenizer.add_special_tokens({'pad_token': '<pad>'}) #HERE\n",
    "# model.resize_token_embeddings(len(tokenizer)) #HERE\n",
    "\n",
    "raw_train_datasets = load_dataset(\"meta-math/MetaMathQA\", split='train[:100]')\n",
    "\n",
    "train_dataset = raw_train_datasets.map(\n",
    "        train_tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=3000,\n",
    "        num_proc=16, # 32\n",
    "        remove_columns=raw_train_datasets.column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "        fn_kwargs={\"tokenizer\": tokenizer, \"query\": \"query\", \"response\": \"response\"}\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = [torch.tensor(x) for x in input_ids]\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = [torch.tensor(x) for x in labels]\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./logs/abrakkk',          \n",
    "    num_train_epochs=1,                   \n",
    "    per_device_train_batch_size=1,       \n",
    "    per_device_eval_batch_size=1,       \n",
    "    gradient_accumulation_steps=1,   \n",
    "    learning_rate=2e-5,               \n",
    "    weight_decay=0.0,                   \n",
    "    warmup_ratio=0.03,                   \n",
    "    lr_scheduler_type=\"cosine\",          \n",
    "    logging_steps=1,                       \n",
    "    fp16=True,\n",
    "    report_to=\"tensorboard\",              \n",
    "    logging_dir='./logs/abra',                 \n",
    "    # use_cpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "data_module = dict(train_dataset=train_dataset, data_collator=data_collator)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "metric = load('accuracy')\n",
    "\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "# Load the accuracy metric\n",
    "metric = load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    predictions = np.argmax(predictions, axis=2).flatten()\n",
    "    \n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    mask = labels != -100\n",
    "    \n",
    "    return metric.compute(predictions=predictions[mask], references=labels[mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(model=model, tokenizer=tokenizer, args=script_args, **data_module)\n",
    "\n",
    "# trainer = Trainer(model=model, args=training_args, **data_module)\n",
    "trainer = Trainer(model=model, args=training_args, compute_metrics=compute_metrics, **data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_dataset': Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 100\n",
      "}), 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "))}\n"
     ]
    }
   ],
   "source": [
    "print(data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9/100 00:35 < 06:42, 0.23 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model.eval()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# model.config.use_cache = True\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# trainer.train()\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_results)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:4073\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4070\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4072\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4073\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4074\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4081\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4083\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:4287\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4284\u001b[0m         all_inputs\u001b[38;5;241m.\u001b[39madd(inputs_decode)\n\u001b[1;32m   4285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4286\u001b[0m     \u001b[38;5;66;03m# Pad labels here, preparing for preprocess_logits_for_metrics in next logits block.\u001b[39;00m\n\u001b[0;32m-> 4287\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4289\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpad_across_processes(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py:2600\u001b[0m, in \u001b[0;36mAccelerator.pad_across_processes\u001b[0;34m(self, tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpad_across_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2568\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2569\u001b[0m \u001b[38;5;124;03m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[1;32m   2570\u001b[0m \u001b[38;5;124;03m    they can safely be gathered.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:408\u001b[0m, in \u001b[0;36mchained_operation.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DistributedOperationException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    410\u001b[0m         operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:678\u001b[0m, in \u001b[0;36mpad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    675\u001b[0m     new_tensor[indices] \u001b[38;5;241m=\u001b[39m tensor\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tensor\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:126\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[1;32m    118\u001b[0m         {\n\u001b[1;32m    119\u001b[0m             k: recursively_apply(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         }\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Only nested list/tuple/dicts of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` should be passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:658\u001b[0m, in \u001b[0;36mpad_across_processes.<locals>._pad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    655\u001b[0m     dim \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# Gather all sizes\u001b[39;00m\n\u001b[0;32m--> 658\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    659\u001b[0m sizes \u001b[38;5;241m=\u001b[39m gather(size)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Then pad to the maximum size\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "# model.eval()\n",
    "# model.config.use_cache = True\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate(eval_dataset=trainer.train_dataset)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 835, 9330, 29901, 13, 29954, 945, 347, 322, 11131, 526, 23906, 3694, 373, 278, 4280, 10694, 29889, 11131, 3060, 15806, 278, 1298, 395, 29896, 29974, 29906, 29875, 1504, 402, 945, 347, 3060, 15806, 15727, 29896, 29974, 29875, 1504, 1128, 2215, 12435, 526, 402, 945, 347, 322, 11131, 29915, 29879, 3291, 29973, 13, 13, 2277, 29937, 24380, 29901, 1576, 5418, 1546, 1023, 3291, 2427, 29916, 29918, 29896, 29892, 29891, 29918, 29896, 1262, 322, 2427, 29916, 29918, 29906, 29892, 29891, 29918, 29906, 1262, 297, 278, 4280, 10694, 338, 2183, 491, 278, 7063, 779, 3676, 8001, 29916, 29918, 29906, 29899, 29916, 29918, 29896, 4887, 29906, 17108, 29891, 29918, 29906, 29899, 29891, 29918, 29896, 4887, 29906, 4311, 13, 797, 445, 1206, 29892, 11131, 29915, 29879, 1298, 338, 2427, 29896, 29892, 29906, 1262, 322, 402, 945, 347, 29915, 29879, 1298, 338, 2427, 29899, 29896, 29892, 29896, 4935, 13, 6295, 278, 5418, 1546, 1009, 3291, 338, 779, 3676, 29912, 3552, 29899, 29896, 6817, 29898, 29896, 876, 29985, 29906, 29974, 3552, 29896, 6817, 29898, 29906, 876, 29985, 29906, 8738, 3676, 29912, 6278, 29906, 4887, 29906, 29974, 6278, 29896, 4887, 29906, 8738, 3676, 29912, 29946, 29974, 29896, 8738, 3676, 29912, 29945, 4311, 13, 8439, 1079, 29892, 402, 945, 347, 322, 11131, 29915, 29879, 3291, 526, 779, 1884, 287, 741, 3676, 29912, 29945, 7920, 10340, 12435, 29889, 13, 1576, 1234, 338, 29901, 320, 3676, 29912, 29945, 29913, 2], [1, 835, 9330, 29901, 13, 5618, 338, 278, 3001, 3438, 310, 10596, 5832, 21083, 363, 599, 4832, 9404, 10769, 373, 278, 5733, 3815, 29892, 13858, 393, 1269, 4847, 6858, 263, 395, 29906, 29945, 432, 261, 7759, 29892, 263, 395, 29896, 29945, 29889, 29906, 29900, 5101, 310, 3273, 29879, 29892, 322, 263, 5101, 310, 577, 4684, 544, 7612, 472, 395, 29953, 29889, 29947, 29900, 29973, 13, 13, 2277, 29937, 24380, 29901, 9760, 4847, 6858, 263, 395, 29906, 29945, 432, 261, 7759, 29892, 263, 395, 29896, 29945, 29889, 29906, 29900, 5101, 310, 3273, 29879, 29892, 322, 263, 5101, 310, 577, 4684, 544, 7612, 472, 395, 29953, 29889, 29947, 29900, 29889, 13, 6295, 278, 3001, 3438, 363, 1269, 4847, 338, 395, 29906, 29945, 718, 395, 29896, 29945, 29889, 29906, 29900, 718, 395, 29953, 29889, 29947, 29900, 353, 395, 29946, 29955, 29889, 13, 23036, 727, 526, 4832, 9404, 10769, 373, 278, 5733, 3815, 29892, 278, 3001, 3438, 363, 599, 310, 963, 338, 29871, 29896, 29953, 334, 395, 29946, 29955, 353, 395, 29955, 29945, 29906, 29889, 13, 4136, 29871, 29955, 29945, 29906, 13, 1576, 1234, 338, 29901, 29871, 29955, 29945, 29906, 2], [1, 835, 9330, 29901, 13, 29928, 9383, 289, 12535, 29871, 29896, 29906, 274, 6926, 363, 670, 9883, 29915, 29879, 12060, 3250, 29889, 18935, 884, 289, 12535, 29871, 29946, 274, 6926, 29892, 541, 263, 371, 921, 1550, 10534, 363, 278, 6263, 304, 1369, 29889, 1670, 526, 29871, 29896, 29945, 274, 6926, 2175, 29889, 1724, 338, 278, 995, 310, 9815, 2286, 921, 29973, 13, 13, 2277, 29937, 24380, 29901, 1762, 4505, 445, 1108, 29892, 591, 817, 304, 8161, 278, 995, 310, 921, 29892, 607, 11524, 278, 1353, 310, 274, 6926, 18935, 263, 371, 1550, 10534, 363, 278, 6263, 304, 1369, 29889, 13, 12024, 29915, 29879, 2867, 1623, 278, 2472, 2183, 29901, 13, 4557, 310, 274, 6926, 289, 12535, 491, 16879, 29901, 29871, 29896, 29906, 13, 4557, 310, 274, 6926, 289, 12535, 491, 18935, 29901, 29871, 29946, 13, 4557, 310, 274, 6926, 2175, 29901, 29871, 29896, 29945, 13, 4806, 508, 731, 701, 278, 6306, 408, 4477, 29901, 13, 4557, 310, 274, 6926, 289, 12535, 491, 16879, 718, 9681, 310, 274, 6926, 289, 12535, 491, 18935, 448, 9681, 310, 274, 6926, 321, 2579, 491, 18935, 353, 9681, 310, 274, 6926, 2175, 13, 29896, 29906, 718, 29871, 29946, 448, 921, 353, 29871, 29896, 29945, 13, 12024, 29915, 29879, 21092, 322, 4505, 363, 921, 29901, 13, 29896, 29953, 448, 921, 353, 29871, 29896, 29945, 13, 1762, 11695, 403, 921, 29892, 591, 23197, 29871, 29896, 29953, 515, 1716, 11192, 310, 278, 6306, 29901, 13, 29896, 29953, 448, 921, 448, 29871, 29896, 29953, 353, 29871, 29896, 29945, 448, 29871, 29896, 29953, 13, 29899, 29916, 353, 448, 29896, 13, 12881, 635, 29892, 591, 22932, 1716, 11192, 310, 278, 6306, 491, 448, 29896, 304, 4505, 363, 921, 29901, 13, 29916, 353, 29871, 29896, 13, 1576, 995, 310, 921, 338, 29871, 29896, 29889, 13, 4136, 29871, 29896, 13, 1576, 1234, 338, 29901, 29871, 29896, 2], [1, 835, 9330, 29901, 13, 18455, 395, 29896, 29900, 29896, 29900, 29896, 29918, 29941, 29938, 304, 263, 2967, 29871, 29896, 29900, 6043, 29889, 13, 13, 2277, 29937, 24380, 17178, 29896, 29900, 29896, 29900, 29896, 29918, 29941, 353, 29871, 29896, 320, 3822, 29871, 29941, 29985, 29946, 718, 29871, 29900, 320, 3822, 29871, 29941, 29985, 29941, 718, 29871, 29896, 320, 3822, 29871, 29941, 29985, 29906, 718, 29871, 29900, 320, 3822, 29871, 29941, 29985, 29896, 718, 29871, 29896, 320, 3822, 29871, 29941, 29985, 29900, 353, 29871, 29947, 29896, 718, 29871, 29929, 718, 29871, 29896, 353, 320, 1884, 287, 29912, 29929, 29896, 4311, 13, 1576, 1234, 338, 29901, 29871, 29929, 29896, 2], [1, 835, 9330, 29901, 13, 29903, 434, 1736, 297, 263, 12529, 322, 1432, 29871, 29941, 29900, 6233, 29892, 263, 4933, 1183, 975, 344, 267, 13880, 29871, 29941, 29900, 508, 29879, 310, 269, 8887, 29889, 1128, 1784, 508, 29879, 310, 269, 8887, 508, 921, 4933, 7738, 297, 29871, 29947, 6199, 29973, 13, 3644, 591, 1073, 278, 1234, 304, 278, 2038, 1139, 338, 29871, 29946, 29947, 29900, 29892, 825, 338, 278, 995, 310, 9815, 2286, 921, 29973, 13, 13, 2277, 29937, 24380, 29901, 4806, 1073, 393, 1432, 29871, 29941, 29900, 6233, 29892, 263, 4933, 13880, 29871, 29941, 29900, 508, 29879, 310, 269, 8887, 29889, 13, 23036, 727, 526, 29871, 29953, 29900, 6233, 297, 385, 7234, 29892, 322, 29871, 29947, 6199, 297, 3001, 29892, 278, 3001, 1353, 310, 6233, 338, 29871, 29953, 29900, 334, 29871, 29947, 353, 29871, 29946, 29947, 29900, 6233, 29889, 13, 3644, 263, 4933, 13880, 29871, 29941, 29900, 508, 29879, 310, 269, 8887, 1432, 29871, 29941, 29900, 6233, 29892, 769, 297, 29871, 29946, 29947, 29900, 6233, 29892, 372, 674, 7738, 313, 29946, 29947, 29900, 29914, 29941, 29900, 29897, 334, 29871, 29941, 29900, 353, 29871, 29946, 29947, 29900, 508, 29879, 310, 269, 8887, 29889, 13, 4806, 526, 2183, 393, 278, 3001, 1353, 310, 508, 29879, 310, 269, 8887, 7371, 338, 29871, 29946, 29947, 29900, 29892, 577, 591, 508, 2436, 29901, 29871, 29946, 29947, 29900, 353, 29871, 29946, 29947, 29900, 334, 921, 29889, 13, 29928, 3640, 292, 1716, 11192, 491, 29871, 29946, 29947, 29900, 29892, 591, 679, 29901, 921, 353, 29871, 29896, 29889, 13, 1576, 995, 310, 921, 338, 29871, 29896, 29889, 13, 4136, 29871, 29896, 13, 1576, 1234, 338, 29901, 29871, 29896, 2], [1, 835, 9330, 29901, 13, 9802, 338, 1321, 5414, 408, 561, 1997, 304, 282, 1351, 263, 716, 4004, 310, 6520, 29889, 450, 6520, 674, 367, 29871, 29906, 29900, 29900, 29900, 6900, 1472, 322, 29871, 29906, 29900, 6900, 9377, 29889, 7806, 534, 2707, 1359, 310, 408, 561, 1997, 674, 4612, 29871, 29947, 29900, 29900, 6862, 6900, 310, 6520, 29889, 960, 1269, 534, 2707, 1359, 21544, 921, 29892, 322, 727, 29915, 29879, 263, 29871, 29906, 29900, 29995, 16538, 8818, 29892, 920, 1568, 674, 4485, 817, 304, 5146, 363, 408, 561, 1997, 29973, 13, 3644, 591, 1073, 278, 1234, 304, 278, 2038, 1139, 338, 29871, 29946, 29945, 29900, 29900, 29892, 825, 338, 278, 995, 310, 9815, 2286, 921, 29973, 13, 13, 2277, 29937, 24380, 29901, 1576, 4038, 310, 278, 6520, 338, 278, 3309, 6674, 2957, 491, 278, 2920, 29901, 29871, 29906, 29900, 29900, 29900, 334, 29871, 29906, 29900, 353, 29871, 29946, 29900, 29892, 29900, 29900, 29900, 6862, 6900, 29889, 13, 9760, 534, 2707, 1359, 310, 408, 561, 1997, 674, 4612, 29871, 29947, 29900, 29900, 6862, 6900, 310, 6520, 29892, 577, 4485, 674, 817, 29871, 29946, 29900, 29892, 29900, 29900, 29900, 847, 29871, 29947, 29900, 29900, 353, 29871, 29945, 29900, 534, 2707, 18132, 310, 408, 561, 1997, 29889, 13, 1576, 3438, 310, 1269, 534, 2707, 1359, 338, 921, 17208, 29889, 13, 1576, 3001, 3438, 310, 278, 408, 561, 1997, 1728, 16538, 8818, 338, 29871, 29945, 29900, 334, 921, 17208, 29889, 13, 1576, 16538, 8818, 338, 29871, 29906, 29900, 29995, 310, 278, 3001, 3438, 29892, 577, 278, 16538, 8818, 5253, 338, 29871, 29900, 29889, 29906, 334, 313, 29945, 29900, 334, 921, 29897, 353, 29871, 29896, 29900, 334, 921, 17208, 29889, 13, 1576, 3001, 3438, 3704, 16538, 8818, 338, 278, 2533, 310, 278, 3438, 1728, 16538, 8818, 322, 278, 16538, 8818, 5253, 29901, 29871, 29945, 29900, 334, 921, 718, 29871, 29896, 29900, 334, 921, 353, 29871, 29953, 29900, 334, 921, 17208, 29889, 13, 4806, 526, 2183, 393, 278, 3001, 3438, 338, 395, 29946, 29945, 29900, 29900, 29892, 577, 591, 508, 2436, 29901, 29871, 29953, 29900, 334, 921, 353, 395, 29946, 29945, 29900, 29900, 29889, 13, 29928, 3640, 292, 1716, 11192, 491, 29871, 29953, 29900, 29892, 591, 679, 29901, 921, 353, 395, 29955, 29945, 29889, 13, 1576, 995, 310, 921, 338, 395, 29955, 29945, 29889, 13, 4136, 29871, 29955, 29945, 13, 1576, 1234, 338, 29901, 29871, 29955, 29945, 2], [1, 835, 9330, 29901, 13, 29923, 3703, 30010, 29879, 11203, 591, 1141, 29879, 29871, 29953, 29941, 24261, 29936, 372, 591, 1141, 29879, 921, 3064, 408, 1568, 408, 16560, 30010, 29879, 11203, 29889, 29871, 323, 12966, 29892, 825, 338, 278, 7688, 310, 278, 26361, 29973, 13, 3644, 591, 1073, 278, 1234, 304, 278, 2038, 1139, 338, 29871, 29955, 29906, 29892, 825, 338, 278, 995, 310, 9815, 2286, 921, 29973, 13, 13, 2277, 29937, 24380, 29901, 4806, 1073, 393, 382, 3703, 29915, 29879, 11203, 591, 1141, 29879, 29871, 29953, 29941, 24261, 29889, 13, 4806, 884, 1073, 393, 382, 3703, 29915, 29879, 11203, 591, 1141, 29879, 921, 3064, 408, 1568, 408, 16560, 29915, 29879, 11203, 29892, 607, 2794, 16560, 29915, 29879, 11203, 591, 1141, 29879, 29871, 29953, 29941, 29914, 29916, 24261, 29889, 13, 1576, 3001, 7688, 310, 278, 26361, 338, 278, 2533, 310, 278, 7688, 310, 382, 3703, 29915, 29879, 11203, 322, 278, 7688, 310, 16560, 29915, 29879, 11203, 29901, 29871, 29953, 29941, 718, 29871, 29953, 29941, 29914, 29916, 29889, 13, 4806, 526, 2183, 393, 278, 3001, 7688, 310, 278, 26361, 338, 29871, 29955, 29906, 24261, 29892, 577, 591, 508, 2436, 29901, 29871, 29953, 29941, 718, 29871, 29953, 29941, 29914, 29916, 353, 29871, 29955, 29906, 29889, 13, 13296, 1747, 363, 921, 29892, 591, 679, 29901, 921, 353, 29871, 29955, 29889, 13, 1576, 995, 310, 921, 338, 29871, 29955, 29889, 13, 4136, 29871, 29955, 13, 1576, 1234, 338, 29901, 29871, 29955, 2], [1, 835, 9330, 29901, 13, 1576, 4726, 310, 3741, 675, 756, 29871, 29946, 29900, 29900, 17774, 29889, 3118, 11582, 310, 278, 4726, 29915, 29879, 17774, 526, 4796, 29889, 3118, 18615, 310, 278, 1661, 29899, 10921, 17774, 505, 263, 3974, 6689, 29889, 1128, 1784, 310, 278, 1661, 29899, 10921, 17774, 437, 451, 505, 263, 3974, 6689, 29973, 13, 13, 2277, 29937, 24380, 29901, 6716, 11582, 310, 278, 4726, 29915, 29879, 17774, 526, 4796, 29892, 577, 727, 526, 29871, 29946, 29900, 29900, 29914, 29946, 353, 29871, 29896, 29900, 29900, 4796, 17774, 29889, 13, 1576, 9886, 1661, 29899, 10921, 17774, 526, 29871, 29946, 29900, 29900, 448, 29871, 29896, 29900, 29900, 353, 29871, 29941, 29900, 29900, 17774, 29889, 13, 6716, 18615, 310, 278, 1661, 29899, 10921, 17774, 505, 263, 3974, 6689, 29892, 577, 727, 526, 29871, 29941, 29900, 29900, 29914, 29945, 353, 29871, 29953, 29900, 1661, 29899, 10921, 17774, 411, 263, 3974, 6689, 29889, 13, 8439, 1079, 29892, 278, 1353, 310, 1661, 29899, 10921, 17774, 1728, 263, 3974, 6689, 338, 29871, 29941, 29900, 29900, 448, 29871, 29953, 29900, 353, 29871, 29906, 29946, 29900, 29889, 13, 4136, 29871, 29906, 29946, 29900, 13, 1576, 1234, 338, 29901, 29871, 29906, 29946, 29900, 2], [1, 835, 9330, 29901, 13, 22930, 1907, 395, 29878, 29938, 322, 395, 29879, 29938, 13100, 297, 874, 873, 29889, 1932, 395, 29878, 29938, 338, 395, 29896, 29906, 29900, 29900, 8209, 395, 29879, 29938, 338, 395, 29900, 29889, 29941, 29945, 7449, 1724, 338, 278, 995, 310, 395, 29879, 29938, 746, 395, 29878, 29938, 338, 395, 29906, 29946, 29900, 29900, 15485, 14657, 596, 1234, 408, 263, 13677, 304, 278, 20471, 10405, 386, 29879, 29889, 13, 13, 2277, 29937, 24380, 29901, 3644, 395, 29878, 29938, 322, 395, 29879, 29938, 13100, 297, 874, 873, 29892, 769, 591, 1073, 393, 395, 29878, 320, 3822, 269, 353, 413, 29938, 363, 777, 4868, 395, 29895, 1504, 13, 4806, 526, 2183, 393, 746, 395, 29878, 29938, 338, 395, 29896, 29906, 29900, 29900, 1628, 395, 29879, 29938, 338, 395, 29900, 29889, 29941, 29945, 1504, 1105, 591, 508, 731, 701, 278, 6306, 29901, 13, 29938, 29896, 29906, 29900, 29900, 320, 3822, 29871, 29900, 29889, 29941, 29945, 353, 413, 29938, 13, 8942, 572, 9215, 29892, 591, 1284, 393, 395, 29895, 353, 29871, 29946, 29906, 29900, 1504, 13, 10454, 591, 508, 671, 445, 995, 310, 395, 29895, 29938, 304, 4505, 363, 395, 29879, 29938, 746, 395, 29878, 29938, 338, 395, 29906, 29946, 29900, 29900, 21063, 13, 29938, 29906, 29946, 29900, 29900, 320, 3822, 269, 353, 29871, 29946, 29906, 29900, 29938, 13, 29928, 3640, 292, 1716, 11192, 491, 395, 29906, 29946, 29900, 29900, 1628, 591, 1284, 393, 395, 29879, 353, 320, 1884, 287, 29912, 29900, 29889, 29896, 29955, 29945, 1042, 304, 278, 20471, 10405, 386, 29879, 29889, 13, 1576, 1234, 338, 29901, 29871, 29900, 29889, 29896, 29955, 29945, 2], [1, 835, 9330, 29901, 13, 29928, 1351, 18093, 29871, 29947, 8277, 1048, 15006, 29892, 29871, 29953, 8277, 1048, 11420, 2913, 29892, 322, 29871, 29941, 8277, 1048, 22983, 304, 3013, 1075, 19587, 975, 278, 8753, 333, 1036, 29889, 7806, 3143, 3438, 395, 29953, 29889, 1128, 1568, 1258, 17841, 18864, 373, 278, 8277, 29973, 13, 13, 2277, 29937, 24380, 29901, 29928, 1351, 18093, 263, 3001, 310, 29871, 29947, 718, 29871, 29953, 718, 29871, 29941, 353, 29871, 29896, 29955, 8277, 29889, 13, 9760, 3143, 3438, 395, 29953, 29892, 577, 17841, 10398, 263, 3001, 310, 29871, 29896, 29955, 921, 395, 29953, 353, 395, 29896, 29900, 29906, 373, 278, 8277, 29889, 13, 4136, 29871, 29896, 29900, 29906, 13, 1576, 1234, 338, 29901, 29871, 29896, 29900, 29906, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(data_module['train_dataset']['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n",
      "187\n",
      "304\n",
      "108\n",
      "272\n",
      "392\n",
      "238\n",
      "196\n",
      "266\n",
      "122\n",
      "83\n",
      "380\n",
      "452\n",
      "130\n",
      "110\n",
      "116\n",
      "203\n",
      "198\n",
      "266\n",
      "306\n",
      "134\n",
      "512\n",
      "250\n",
      "175\n",
      "167\n",
      "340\n",
      "126\n",
      "115\n",
      "161\n",
      "97\n",
      "189\n",
      "248\n",
      "162\n",
      "178\n",
      "151\n",
      "145\n",
      "384\n",
      "197\n",
      "190\n",
      "440\n",
      "343\n",
      "171\n",
      "383\n",
      "392\n",
      "162\n",
      "268\n",
      "157\n",
      "268\n",
      "292\n",
      "315\n",
      "184\n",
      "415\n",
      "152\n",
      "199\n",
      "374\n",
      "168\n",
      "258\n",
      "329\n",
      "288\n",
      "262\n",
      "260\n",
      "319\n",
      "260\n",
      "253\n",
      "209\n",
      "454\n",
      "227\n",
      "192\n",
      "277\n",
      "244\n",
      "145\n",
      "291\n",
      "200\n",
      "370\n",
      "104\n",
      "180\n",
      "158\n",
      "210\n",
      "100\n",
      "222\n",
      "189\n",
      "441\n",
      "273\n",
      "136\n",
      "210\n",
      "283\n",
      "146\n",
      "362\n",
      "172\n",
      "512\n",
      "168\n",
      "146\n",
      "209\n",
      "302\n",
      "91\n",
      "196\n",
      "256\n",
      "141\n",
      "257\n",
      "197\n"
     ]
    }
   ],
   "source": [
    "for e in trainer.train_dataset:\n",
    "    print(len(e['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
